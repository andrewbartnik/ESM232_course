---
title: 'Choosing models - evaluating performance'
output:
  slidy_presentation:
    highlight: pygments
  html_document: default
  pdf_document: default
  ioslides_presentation:
    highlight: pygments
  beamer_presentation:
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(sensitivity)
library(tidyverse)
library(deSolve)
library(lhs)
library(purrr)
library(ggpubr)
```

# Background material

See materials in choosingmodels.pdf

Some examples of model evaluation using results
from a hydrologic model applied to a Sierra watershed

# Graph

```{r simple}

sager = read.table("../Data/sager.txt", header=T)
head(sager)

# add date
sager = sager %>% mutate(date = paste(day,month,year, sep="/"))
sager$date = as.Date(sager$date,"%d/%m/%Y")

# plot
sagerl = sager %>% pivot_longer(cols=c("model","obs"), names_to="source",
                                  values_to="flow")

# basic plot
ggplot(sagerl, aes(date, flow, col=source, linetype=source))+geom_line()

# change access to get a closer look at performance at low values
# when you have high dynamic range (lots of large and small values), taking log can help
# with visualization
ggplot(sagerl, aes(date, flow, col=source, linetype=source))+geom_line()+scale_y_continuous(trans="log")+labs(y="streamflow mm/day")

# look at it another way
ggplot(sager, aes(obs, model))+geom_point()+geom_abline(intercept=0, slope=1, col="red")


```

# Measure Performance using different metrics

```{r}


source("../R/nse.R")

source("../R/relerr.R")


source("../R/cper.R")

nse
relerr
cper
nse(m=sager$model, o=sager$obs)

relerr(m=sager$model, o=sager$obs)*100

cper(m=sager$model, o=sager$obs, weight.nse=0.8)


```

# Scale and subsetting

Performance also depends on the 'what' you are evaluating

  * time steps (annual, daily, monthly)
  
  * selection of particular periods of time
  

  
```{r}
# try a different time step
sager_wy = sager %>% group_by(wy) %>% summarize(model=sum(model), obs=sum(obs))

nse(sager_wy$model, sager_wy$obs)
cper(m=sager_wy$model, o=sager_wy$obs, weight.nse=0.8)

# just look at august flow
# first sum by month
tmp = sager %>% group_by(month, year) %>% summarize(model=sum(model), obs=sum(obs))

# now extract august
sager_aug = subset(tmp, month==8)
cor(sager_aug$model, sager_aug$obs)


```

# In Class Exercise

* Read in the *sager.txt* data

* Think of a new metric that might be interesting from a particular environmental context

* Code that metric as a function - and then apply it

# Using multiple metrics

```{r}

# turn your evaluation metric into a function
source("../R/compute_lowflowmetrics.R")
compute_lowflowmetrics
compute_lowflowmetrics(m=sager$model,o=sager$obs, month=sager$month, day=sager$day, year=sager$year, wy=sager$wy)

# use different low flow months
compute_lowflowmetrics(m=sager$model,o=sager$obs, month=sager$month, day=sager$day, year=sager$year, wy=sager$wy, low_flow_months = c(7:9))

```
  

# create a combined metric
```{r}

perf = compute_lowflowmetrics(m=sager$model,o=sager$obs, month=sager$month, day=sager$day, year=sager$year, wy=sager$wy, low_flow_months = c(7:9))

perf = as.data.frame((perf))
# remember you want error to be low but correlation to be high 
# so we need to transform in some way

# normalize by max error = if error is greater than this we don't care
# many ideas -  maybe 50% of mean daily summer observed low flow
tmp = sager %>% subset(month %in% c(7:9)) 
errmax = mean(tmp$obs)*0.5

perf = perf %>% mutate(annual_min_err_trans = max(0,(1-abs(annual_min_err/errmax) )))
      
# for monthly we can do a simpler thing to find maximum allowable errror   
tmp = sager %>% subset(month %in% c(7:9)) %>% group_by(wy, month) %>% summarize(obs=sum(obs))

errmax = mean(tmp$obs)*0.5
 
perf = perf %>% mutate(low_month_err_trans = max(0,(1-abs(low_month_err/errmax) )))

# now we have 4 measures that we can combine together

perf = perf %>% mutate(combined = (annual_min_cor + annual_min_err_trans + low_month_err_trans + low_month_cor)/4)
perf
# or weight differently - we know that minimum flows are hard to get to weight those differently

perf = perf %>% mutate(combined2 = 0.1*annual_min_cor + 0.1*annual_min_err_trans + 0.4*low_month_err_trans+ 0.4*low_month_cor)

perf

# easier to put all this in a function


```
# Come up with a combined metric that you think is interesting 



# Calibration

Calibration is picking parameter sets based on performance evaluation

Apply metrics over multiple outputs (generated by running across many parameters sets) - like we've done in our sensitivity analysis work

Example - a dataset where each column
is a different model run for Sagehen Creek
(using different parameters) - don't worry about the parameters for now

* sagerm.txt



```{r multipel}

# multiple results - lets say we've run the model for multiple years, each column
# is streamflow for a different parameter set
msage = read.table("../Data/sagerm.txt", header=T)

# keep track of number of simulations and use as a column names
nsim = ncol(msage)
snames = sprintf("S%d",seq(from=1, to=nsim))
colnames(msage)=snames
# lets say we know the start date from our earlier output
msage$date = sager$date
head(msage)
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy



# lets add observed
msage = left_join(msage, sager[,c("obs","date")], by=c("date"))

# and we still have observed data from above


# how can we plot all results - lets plot water year 1970 otherwise its hard to see
msagel = msage %>% pivot_longer(cols=!c(date, month, year, day,wy), names_to="run", values_to="flow")

p1=ggplot(subset(msagel, wy == 1970), aes(as.Date(date), flow, col=run))+geom_line()+theme(legend.position = "none")
p1
# lets add observed streamflow
p1+geom_line(data=subset(sager, wy == 1970), aes(as.Date(date), obs), size=2, col="black", linetype=2)+labs(y="Streamflow", x="Date")

# subset for calibration

short_msage = subset(msage, wy < 1975)

# compute performance measures for all output
res = short_msage %>% select(!c("date","month","year","day","wy","obs")) %>%
      map_dbl(nse, short_msage$obs )

head(res)



# another example using our low flow statistics
# use apply to compute for all the data
source("../R/compute_lowflowmetrics_all.R")
res = short_msage %>% select(-date, -month, -day, -year, -wy, -obs ) %>% map_df(compute_lowflowmetrics_all, o=short_msage$obs, month=short_msage$month, day=short_msage$day, year=short_msage$year, wy=short_msage$wy)


# interesting to look at range of metrics - could use this to decide on
# acceptable values
summary(res)
# we can add a row that links with simulations
res$sim = snames

# graph range of performance measures
resl = res %>% pivot_longer(-sim, names_to="metric", values_to="value")
ggplot(resl, aes(metric, value))+geom_boxplot()+facet_wrap(~metric, scales="free")


# best one

best = res[which.max(res$combined),]
msagel  =  msage %>% pivot_longer(cols=!c(date, month, year, day,wy, obs), names_to="sim", values_to="flow")
 
# first assume we have data for calibration, and then we are running the model forward
# so we can look at the full time series
 ggplot(msage, aes(date, msage[,best$sim])) + geom_line()+geom_line(aes(date, obs), col="red") 
 bestrun = msagel[,best$sim]
 worst = res[which.min(res$combined),]
 worstrun = msagel[,worst$sim]
 
 compruns = msage %>% select(best$sim, worst$sim, date, obs, month, day, year, wy)
 compruns = subset(compruns, wy > 1970)
 compruns_mwy = compruns %>% select(-c(day,date, year)) %>% group_by(month, wy) %>% summarize(across(everything(), mean))
 
 compruns_mwyl = compruns_mwy %>% pivot_longer(cols=!c(month,wy), names_to="sim", values_to="flow")
 compruns_mwyl %>% subset(month==8) %>% ggplot(aes(sim,flow ))+geom_boxplot()
 




```

Before Tuesday - create your own metric to evaluate streamflow performane
apply it to the Sagehen streamflow data

You can use compute_performance_metrics.Rmd in ESM232_Examples as a guide - 

Add the simulation run is the best one based on your metric to the Gauchospace survey
