---
title: "Sensitivity Analysis"
output:
  slidy_presentation:
    highlight: pygments
  html_document: default
  pdf_document: default
  ioslides_presentation:
    highlight: pygments
  beamer_presentation:
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(sensitivity)
library(tidyverse)
library(pse)
library(purrr)
```

#  Sensitivity Analysis 

Many approaches (entire text books)

Two main classes
  
* global simultaneous: vary all parameters over possible ranges

* local parameter specific: hold all other parameters content and then vary
    
Challenge is sampling parameter uncertainty space and balancing this against computational limits

Optimization - special type of sensitivity analysis

---

#  Single Parameter Sensitivity

Vary one parameter but hold the others constant

Useful for focused analysis but..

* sensitivity to a parameter may change as a function of other parameters

Consider the sensitivity of a seasonal snow accumulation and melt model to both temperature and radiation

* for high temperatures, radiation may not impact rates substantially since there is less snow

* for lower temperature, radiation may matter much more - so temperatures related to radiation absorption (e.g albedo) will have a greater impact on output

---

#  Steps in Sensitivity Analysis 

Define the range (pdf) of input parameters

Define the outputs to be considered (e.g if you had streamflow are you looking at daily, max, min, annual)

Sample the pdf of input parameters and use this sample to run the model - repeat many times

Graph the results

Quantify sensitivity

---

#  Sensitivity Analysis: What ranges should I vary my parameters over!

* range (min, max, middle values)

* pdf  (probability density function)

* If parameters are physically based this can come from literature (e.g range of snow albedo’s, range of hydraulic conductivity in a soil )

* If parameter’s are estimated (e.g a regression slope, coefficients from another model) then statistics such as confidence bounds can help  you to define the ranges

---

#  Approaches to sampling parameter space 

Random - Monte Carlo

Latin Hypercube

Sobel

Difference is in how you sample parameter space - tradeoffs with efficiency, likelihood of capturing responses from rarer parameters

---

#  Latin Hyper Cube 

generating random samples from equally probability intervals
![](lecture7_sensitivity/assets/img/image3.png)

---

#  Tools in R 

PSE (Parameter Space Exploration with Latin Hypercubes)
Library for Latin Hypercube Sampling and Sensitivity Analysis

*Install PSE library*


# What we need to use the LHS function


* parameter names

* parameter distributions (type and parameters)


“qnorm”,”qunif”, “qlnorm”,”qgamma” many others]

[More Distributions and Parameters](http://www.stat.umn.edu/geyer/old/5101/rlook.html)

* number of parameter sets

* a few summary metrics to describe you output

* *LHS* will return samples using Latin Hypercube Sampling

* *LHS* returns this in an *LHS* type object that contains useful information


# Example of using LHS for sensitivity analysis

Lets look at our almond yield example

```{r LHS}
# for formal sensitivity analysis it is useful to describe output in
# several summary statistics - how about mean, max and min yield
source("../R/compute_almond_yield.R")


# Lets consider 3 of the parameters....
factors = c("Tmincoeff1", "Tmincoeff2", "Pcoeff2")

# Decide How many parameter sets to run
nsets=100

# choose distributions for parameters - this would come from
# what you know about the likely range of variation
q = c("qnorm", "qnorm", "qunif")
q.arg = list(list(mean=-0.015,sd=0.005), list(mean=-0.0046, sd=0.001), list(min=0.00429, max=0.00431))

# generate samples from LHS
sens_almond = LHS(NULL,factors,nsets,q,q.arg)
sens_pars = get.data(sens_almond)
head(sens_pars)


```

# Run model for parameter sets

* We will do this in R

* You could however output parameter sets and run your model in its native environment

* We then *tell* the results of the model to our *LHS* object that we generated when we generated parameter sets 

* To *tell* results must be a data.frame or matrix

# Examining Output

Sensitivity of What?

If your model is estimating a single value, you are done

* long term mean almond yield anomoly
*  mean profit from solar

But models are often estimating multiple values

* streamflow
* almond yield anomoly for multiple years
  
In that case to quantify sensitivity you need summary metrics

* mean
* max
* min
* variance

Which one depends on what you care about


```{r almondsens}


# read in the input data
SB=read.table("../data/clim.txt")
clim= SB



# lets now run our model for all of the parameters generated by LHS
# pmap is useful here - it is a map function that uses the actual names of input parameters

yields = sens_pars %>% pmap(compute_almond_yield,clim=clim)

# notice that what pmap returns is a list 
head(yields)

# turn results in to a dataframe for easy display/analysis
yieldsd = yields %>% map_dfr(`[`,c("maxyield","minyield","meanyield"))

# to take advantage of LHS/pse functions for 
# plotting interesting information we can send results back - 
# results need to be in a matrix
# each column is a different parameter set - we can use transpose (t)
# and as.matrix to get there

# tell is what links output to original LHS object

sens_almond = pse::tell(sens_almond, t(as.matrix(yieldsd)),
                        res.names=c("maxyield","minyield","meanyield"))
```


#  Plotting 

Plot relationship between parameter and output
to understand how uncertainty in parameter impacts the output to determine over what ranges of the parameter uncertainty is most important (biggest effect)


Use a box plot (of output)
to graphically show the impact of uncertainty on output of interest

---

```{r senplot}
# now we use built in LHS functions to analyze parameter sensitivity
pse::plotscatter(sens_almond, col="blue", cex=5)


# we can also plot results in interesting ways
# turn sens_almond into a data frame - easier access to R plotting functions

ggplot(yieldsd, aes(minyield, maxyield))+geom_point()+labs(y="Max Yield (as anomoly)", "Min Yield (as anomoly")


# add uncertainty bounds on our estimates
tmp = yieldsd %>% gather(value="value", key="yield")
ggplot(tmp, aes(yield, value, col=yield))+geom_boxplot()+
  labs(y="Yield (as anomoly)")

# note that you don't see the ranges because of the scale (min yield anomoly much smaller than max) - here's a more informative way to graph
ggplot(tmp, aes(yield, value, col=yield))+
  geom_boxplot()+labs(y="Yield (as anomoly)")+
  facet_wrap(~yield, scales="free" )




```

#  Quantifying Sensitivity 

If relationships between output and input are close to linear,

* Correlation coefficient between output and each input (separately)

* Partial correlation coefficient (PCC) for multiple parameters; 

      * Correlation after effects of other parameters accounted for
      
      * Correlation of X and Y given Z
      
      * Computed as the correlation of the *residuals* from linear regression of
      
          * X with Z
          
          * Y with Z

We can essentially fit a regression relationship between input and output - and  slope is a measure of sensitivity (standardized regression coefficients)
\begin{align*}
   y = \beta * x + \alpha  \\
  {\beta}_{std} = \beta * \frac{s_x}{s_y} 
\end{align*}

where $s_x$ and $s_y$ are standard deviations of x and y

#  Quantifying Sensitivity 

Non-linear relationship, but monotonic between x and y - you can rank transform x and y to developed 

* Spearman Correlation Coefficient

* Partial Rank Correlation Coefficient

# PSE:TELL object  has partial rank correlation coefficients

```{r quantifying}
# prcc's automatically generated and easy to plot
pse::plotprcc(sens_almond)

# PRCC for all 3 output metrix
sens_almond$res.names
sens_almond$prcc

# correlation coefficient
# compare PRCC with first correlation coefficient
# recall
head(yieldsd)
sens_almond$prcc[1]

# we can still use our sens_pars data frame - rows of parameters will be
# match rows in the output from our use of pmap to run the model for all 
# parameters
cor(yieldsd$maxyield, sens_pars$Tmincoeff1, method="spearman")
cor(yieldsd$maxyield, sens_pars$Tmincoeff2, method="spearman")
cor(yieldsd$maxyield, sens_pars$Pcoeff2, method = "spearman")
```

---

#  Other Formal Sensitivity Approaches


Sobol method

Fourier Amplitude Sensitivity Test

* more efficient parameter space sampling
* estimates of parameter contributions that can be more informative 

---

# Larger Complex Models

Only do sensitivity on some inputs/parameters


Use LHS to generate samples

Run model for samples

Generate Summary Statistics

Add these to LHS object to use plotscatter, plotprcc (and other functions in “pse” package

---

# More Information on Sensitivity Analysis

[Applying Sensitivity Analysis in Biology] (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570191/)

---

#  Steps in Sensitivity Analysis 

Define the range (pdf) of input parameters

Define the outputs to be considered (e.g if you had streamflow are you looking at daily, max, min, annual ?)

Sample from pdf of input parameters - we use *LHS* for this

Run the model for each sample

Store results in *LHS* object (if use used *LHS* to generate)

Graph the results

Quantify sensitivity


Make sure you install the **pse** library

---

# Assignment

With a new group: Sensitivity Analysis

Often when we are estimating vegetation or crop water use we need to know the atmospheric conductance - which is essentially how easily water diffuses into the air and depends largely on windspeed (you get more evaporation in windier conditions) Atmospheric conductance is also influenced by the vegetation itself and the turbulence it creates

1. Code a function to compute atmospheric conductance $C_{at}$ (how easily vapor diffuses from vegetation surfaces)


You can estimate
$$
C_{at} = \frac{v_m}{6.25*{ln(\frac{z_m-z_d}{z_0})}^2}
$$
$$
z_d = k_d*h
$$
$$
z_0 = k_0*h
$$


$z_m$ is the height at which windspeed is measured - must be higher than the vegetation (cm), it is usually measured 200 cm  above the vegetation

$h$ is vegetation height (cm)

$v$ is windspeed (cm/s)

Typical values if $k_d$ and $k_o$ are 0.7 and 0.1 respectively (so use those as defaults)

2.  Run your model

You are estimating the atmospheric conductance for a forest that is 10 m high (the accuracy of that measurement is +/- 0.5 m )
Windspeeds  $v$ in this region are normally distributed with a mean of  250 cm/s with a standard deviation of 30 cm/s

Come up with a single  estimate of atmospheric conductance for this forest

3. Now do a sensitivity analysis as follows

 Consider the sensitivity of your estimate to uncertainty in the following parameters and inputs
  
  * $h$ 
  
  * $k_d$
  
  * $k_0$
  
  * $v$
  
Windspeeds $v$ are normally distributed with a mean of  250 cm/s with a standard deviation of 30 cm/s

For vegetation height assume that height is somewhere between 9.5 and 10.5 m (but any value in that range is equally likely)

For the $k_d$ and $k_0$ parameters you can assume that they are normally distributed with standard deviation of 1% of their default values

a) use LHS to generate parameter values for the 4 parameters
b) run you atmospheric conductance model for these parameters and return aerodynamic conductances 

c) Plot conductance estimates in a way that accounts for parameter uncertainty

d)  Plot conductance estimates against each of your parameters

e) Estimate the Partial Rank Correlation Coefficients

f) Discuss what your results tell you about how aerodynamic conductance? What does it suggest about what you should focus on if you want to reduce uncertainty in aerodymaic conductance estimates? Does this tell you anything about the sensitivity of plant water use to climate change? 

